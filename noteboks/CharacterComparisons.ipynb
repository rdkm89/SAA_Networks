{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count and metric measure rankings\n",
    "\n",
    "Script for generating ranking statistics for Early Modern plays.\n",
    "\n",
    "NB: this script depends on the output from NetworkAnalysis.ipynb \n",
    "    &&\n",
    "    Plays need to have a .Gephi file with node metrics.\n",
    "    \n",
    "Long term goals:\n",
    "\n",
    "    -  Automate for full folder of plays.\n",
    "    -  Remove Gephi from the workflow and generate node stats using Networkx\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Preflight checks\n",
    "\n",
    "Import packages and define functions for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages, modules\n",
    "#import os\n",
    "#from os import listdir\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def list_xmlfiles(directory):\n",
    "    \"\"\"\n",
    "    Return a list of filenames ending in '.txt' in DIRECTORY.\n",
    "    Not strictly necessary but will be useful if we try to scale.\n",
    "    \"\"\"\n",
    "    xmlfiles = []\n",
    "    for filename in listdir(directory):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            xmlfiles.append(filename)\n",
    "    return xmlfiles\n",
    "\n",
    "def list_textfiles(directory):\n",
    "    \"\"\"\n",
    "    Return a list of filenames ending in '.txt' in DIRECTORY.\n",
    "    Not strictly necessary but will be useful if we try to scale.\n",
    "    \"\"\"\n",
    "    textfiles = []\n",
    "    for filename in listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            textfiles.append(filename)\n",
    "    return textfiles\n",
    "\n",
    "def count_totals(character_list):\n",
    "    \"\"\"\n",
    "    Function to count the total number of speech acts and lines per character in each play\n",
    "    \"\"\"\n",
    "    counts = []\n",
    "    \n",
    "    for character in list(unique):\n",
    "        lines = [[line.text for line in test(['l','p'])] for test in soup.findAll(who=character)]\n",
    "        words = [[word.replace('\\n', ' ').replace('\\r', '') for word in words] for words in lines]\n",
    "        #l = [[[((len(re.findall(r'\\w+', s)))) for s in i] for i in item] for item in words]\n",
    "        \n",
    "        x = []\n",
    "        for item in words:\n",
    "            for s in item:\n",
    "                x.append(len(re.findall(r'\\w+', s)))\n",
    "        \n",
    "        speech_acts = len(lines)\n",
    "        total_words = sum(x)\n",
    "        totals = (character, speech_acts, total_words)\n",
    "        counts.append(totals)\n",
    "        \n",
    "    df = pd.DataFrame(counts, columns=[\"character\", \"lines\", \"words\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def total_rankings(df):\n",
    "    \"\"\"\n",
    "    Create count rankings based on word and line lengths.\n",
    "    \"\"\"\n",
    "    df[\"line_rank\"] = df[\"lines\"].sort_values(ascending=False).rank(method='dense', ascending=False).astype(int)\n",
    "    df[\"word_rank\"] = df[\"words\"].sort_values(ascending=False).rank(method='dense', ascending=False).astype(int)\n",
    "    df[\"count_rank\"] = ((df[\"line_rank\"] + df[\"word_rank\"])/2).astype(int)\n",
    "    return df\n",
    "\n",
    "def metric_rankings(df):\n",
    "    \"\"\"\n",
    "    Create metrics rankings based on node metrics from .Gephi file\n",
    "    \n",
    "    I don't like this function very much. It's too pandas-y. But it works.\n",
    "    \"\"\"\n",
    "    df[\"WD_rank\"] = df[\"Weighted Degree\"].sort_values(ascending=False).rank(method='dense', ascending=False).astype(int)\n",
    "    df[\"EC_rank\"] = df[\"eigencentrality\"].sort_values(ascending=False).rank(method='dense', ascending=False).astype(int)\n",
    "    df[\"degree_rank\"] = df[\"Degree\"].sort_values(ascending=False).rank(method='dense', ascending=False).astype(int)\n",
    "    df[\"BC_rank\"] = df[\"betweenesscentrality\"].sort_values(ascending=False).rank(method='dense', ascending=False).astype(int)\n",
    "    df[\"metrics_rank\"] = ((df[\"WD_rank\"] + df[\"EC_rank\"] + df[\"degree_rank\"] + df[\"BC_rank\"])/4).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Calculate count measures\n",
    "\n",
    "First, we read in the Early Print XML play. Then we create idList, which is the list of all characters in the play. We then take the unique set of these characters and feed that into the count_totals function.\n",
    "\n",
    "This returns a dataframe called **_totals_** which contains a list of count measures for each character in the play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in plays and create BeautifulSoup object\n",
    "filename = \"/path/to/play.xml\"\n",
    "with open(filename, 'r') as file: \n",
    "    raw = file.read()\n",
    "    soup = BeautifulSoup(raw, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of characters based on lines\n",
    "idList = []\n",
    "for a in soup.findAll('sp'):\n",
    "    if 'who' in a.attrs.keys():\n",
    "        idList.append(a.attrs['who'])\n",
    "\n",
    "# Only unique characters\n",
    "unique = set(idList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count totals\n",
    "totals = count_totals(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b - Cleanup tables and rank measures\n",
    "\n",
    "There are still some errors in these tables that require a little fiddling around. The following lines are meant only as examples of the kind of cleaning up that can be (and has been) performed on the **_totals_** tables.\n",
    "\n",
    "The cleaned up **_totals_** table is then sent to the total_rankings function, returning a new dataframe called **_count-ranks_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove TCP string\n",
    "totals[\"character\"] = totals['character'].str.replace('A77565_01-','')\n",
    "# Change spellings and recount the totals\n",
    "totals[\"character\"] = totals[\"character\"].str.replace('phib','phebe')\n",
    "totals = totals.groupby('character').sum().reset_index()\n",
    "# Delete characters\n",
    "totals = totals.drop([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate + save count ranks\n",
    "count_ranks = total_rankings(totals)\n",
    "file1 = \"/path/to/ranked_counts.csv\"\n",
    "count_ranks.to_csv(file1, header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Calculate metric measures\n",
    "\n",
    "We now import the table of node metrics generated using Gephi. This table is then sent to the **_metric_ranking_** function which returns a dataframe called **_metric-ranks_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "gephi = \"/Users/au564346/Desktop/gephi_metrics_brome.csv\"\n",
    "metrics = pd.read_csv(gephi)\n",
    "metrics = metrics[[\"Id\", \"Degree\", \"Weighted Degree\", \"eigencentrality\", \"betweenesscentrality\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for consistency\n",
    "len(metrics) == len(count_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate + save ranked metric measures\n",
    "metric_ranks = metric_rankings(metrics)\n",
    "metric_ranks.to_csv(\"/path/to/ranked_metrics.csv\",\n",
    "             header=True, sep=\"\\t\")\n",
    "metric_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Combine tables\n",
    "\n",
    "Firstly, we create an abridged table which has only the average count and metrics rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save abridged ranks\n",
    "ranks = count_ranks.merge(metric_ranks, left_on='character', right_on='Id')\n",
    "ranks = ranks[[\"character\", \"count_rank\", \"metrics_rank\"]]\n",
    "ranks.to_csv(\"/paht/to/ranked.csv\",\n",
    "            header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a larger table that brings together all of our desired metrics into a single table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ranks = count_ranks.merge(metric_ranks, left_on='character', right_on='Id')\n",
    "all_ranks = all_ranks[[\"character\", \"lines\", \"words\", \"Degree\", \"Weighted Degree\",\n",
    "                        \"eigencentrality\", \"betweenesscentrality\", \"line_rank\", \"word_rank\", \"degree_rank\", \n",
    "                           \"WD_rank\",\"BC_rank\", \"EC_rank\", \"count_rank\", \"metrics_rank\"]]\n",
    "all_ranks.to_csv(\"/path/to/complete_rankings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we calculate the Spearman's Rho on the average count and metric rankings, in order to see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate + save spearman's rho\n",
    "corr = ranks.corr(method='spearman')\n",
    "corr.to_csv(\"/path/to/correlations.csv\",\n",
    "            header=True, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
